{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NNsRLtutorial.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "<h1> Approximating value function with neural networks </h1>\n",
        "\n",
        "In those two tutorials, we will be approximating state-action value function $Q(s,a)$ by a neural network. The environment we will use is the cart-pole environment of the OpenAI gym library. In this environment, the goal is to balance an inverse pendulum. Once the pendulum fails, the episode terminates. As long as the pendulum is more or less upright, you obtain reward $+1$ for each step. The environment is considered solved once you can balance the pendulum for $200$ steps or more. Actions are $0$ and $1$ for pushing the cart to the left or right. The state-space contains four continuous variables.\n",
        "\n",
        "See [https://github.com/openai/gym/blob/master/gym/envs/classic_control/cartpole.py](https://github.com/openai/gym/blob/master/gym/envs/classic_control/cartpole.py) and [https://gym.openai.com/envs/CartPole-v0/](https://gym.openai.com/envs/CartPole-v0/) for more details."
      ],
      "metadata": {
        "id": "TBcMhTL7sice"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jbsiTJHPsdIT"
      },
      "outputs": [],
      "source": [
        "%matplotlib inline\n",
        "import gym\n",
        "from gym.wrappers import Monitor\n",
        "\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "from PIL import Image\n",
        "from IPython import display as ipythondisplay\n",
        "\n",
        "!apt-get install x11-utils > /dev/null 2>&1 \n",
        "!pip install pyglet > /dev/null 2>&1 \n",
        "\n",
        "!pip install gym pyvirtualdisplay > /dev/null 2>&1\n",
        "!apt-get install -y xvfb python-opengl ffmpeg > /dev/null 2>&1\n",
        "\n",
        "from pyvirtualdisplay import Display\n",
        "display = Display(visible=0, size=(1400, 900))\n",
        "display.start()\n",
        "\n",
        "env = gym.make('CartPole-v0')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We will break the whole algorithm into several blocks we will implement separately. The first and the simplest one is the replay memory. Replay memory is a cyclic buffer that will be used to store triplets of state, action, and the sampled $Q(s,a)$."
      ],
      "metadata": {
        "id": "DNo1r8rb3bPm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ReplayMemory:\n",
        "  def __init__(self, capacity):\n",
        "    # TODO: create a cyclic buffer of a given size\n",
        "    self.capacity = capacity\n",
        "    self.memory = #TODO\n",
        "    pass\n",
        "\n",
        "  def put(self, state, action, q_state_action):\n",
        "    # TODO store a sample into the buffer\n",
        "    pass\n",
        "\n",
        "  def sample(self, number):\n",
        "    # TODO samples a given number of samples uniformly i.i.d. from the buffer\n",
        "    return []\n",
        "\n",
        "  def size(self):\n",
        "    # TODO gets the actual size of the buffer\n",
        "    # we will need this method later ...\n",
        "    return 0"
      ],
      "metadata": {
        "id": "zHq6PGlA3aiL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "In the next step, we need to create a simple neural network to approximate the $Q$-values. If you do not know how to create a simple neural network, try visiting, for example, [this tutorial](https://towardsdatascience.com/building-neural-network-using-pytorch-84f6e75f9a)."
      ],
      "metadata": {
        "id": "Ft3o6uwv7NTw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch import nn\n",
        "\n",
        "class Network(nn.Module):\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "    self.hidden = #TODO\n",
        "        \n",
        "  def forward(self, x):\n",
        "    # TODO implement the forward pass through the network.\n",
        "    x = self.hidden(x)\n",
        "\n",
        "    # TODO\n",
        "\n",
        "    return x"
      ],
      "metadata": {
        "id": "oRZR30g-7dP3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next, we will create a helper class <code>Model</code> that we will use to access the network. We will implement the greedy and $\\varepsilon$-greedy policies, together with the optimization step."
      ],
      "metadata": {
        "id": "4X1jH7c5OOhX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "import random\n",
        "\n",
        "class Model:\n",
        "  def __init__(self) -> None:\n",
        "      super().__init__()\n",
        "      self.network = Network()\n",
        "      # we will need a proper loss function and an optimizer\n",
        "      # you may get inspired here:\n",
        "      # https://pytorch.org/tutorials/intermediate/reinforcement_q_learning.html\n",
        "      # or select one from the list here:\n",
        "      # https://neptune.ai/blog/pytorch-loss-functions\n",
        "      # https://pytorch.org/docs/stable/optim.html\n",
        "      self.criterion = # TODO choose your loss\n",
        "      self.optimizer = # TODO chose your optimizer\n",
        "\n",
        "  def greedy_policy(self, observation):\n",
        "    # TODO implement the greedy policy, return 0/1\n",
        "    with torch.no_grad(): \n",
        "      # TODO\n",
        "      return 0\n",
        "\n",
        "  def greedy_q_value(self, observation):\n",
        "    # TODO implement the method to estimate U(s) as max_a Q(s,a)\n",
        "    # we will need this method to calculate the target value to learn\n",
        "    with torch.no_grad(): \n",
        "      return 0.0\n",
        "\n",
        "  def eps_greedy(self, observation, epsilon):\n",
        "    # TODO implement epsilon-greedy policy\n",
        "    return 0\n",
        "\n",
        "  def optimize_batch(self, replay_memory, batch_size):\n",
        "    # in this method, we will actually train the neural network\n",
        "\n",
        "    # if there are not enough samples in the history, skip learning\n",
        "    if(replay_memory.size() < batch_size): return\n",
        "\n",
        "    sample = replay_memory.sample(batch_size)\n",
        "\n",
        "    prediction_vals = # TODO calculate predicted values by the netwrok on the sample\n",
        "    targets = # TODO this is what the learning should have predicted\n",
        "\n",
        "    # actual training\n",
        "    loss = self.criterion(prediction_vals, targets)\n",
        "    self.optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "  \n",
        "    for param in self.network.parameters():\n",
        "      param.grad.data.clamp_(-1, 1)\n",
        "    self.optimizer.step()"
      ],
      "metadata": {
        "id": "IQf_PUgNmVbT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "Finally, we are ready to put everything together in the OpenAI gym library."
      ],
      "metadata": {
        "id": "bJUHGCWAzp-8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO feel free to modify\n",
        "NUMBER_OF_EPISODES = 1000\n",
        "REPLAY_MEMORY_SIZE = 3000\n",
        "BATCH_SIZE = 256\n",
        "\n",
        "model = Model()\n",
        "history = ReplayMemory(REPLAY_MEMORY_SIZE)\n",
        "\n",
        "for e in range(NUMBER_OF_EPISODES):\n",
        "  last_obs = env.reset()\n",
        "\n",
        "  for i in range(500): # the environment won't let us more than 500 actions\n",
        "    epsilon = # TODO calculate the epsilon\n",
        "\n",
        "    # do the step\n",
        "    last_action = model.eps_greedy(last_obs, epsilon)\n",
        "    obs, reward, done, _ = env.step(last_action)\n",
        "\n",
        "    # store into history and optimize the model\n",
        "    td_target = # TODO calculate the sampled value\n",
        "    history.put(last_obs, last_action, td_target)\n",
        "\n",
        "    model.optimize_batch(history, BATCH_SIZE)\n",
        "\n",
        "    last_obs = obs\n",
        "\n",
        "    # break and print how long we were able to balance the pendulum\n",
        "    if done:\n",
        "      print(i) # to know the length of the episode\n",
        "      break"
      ],
      "metadata": {
        "id": "_s8JK61ZmgbR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, if we want to test our approach, we can visualize it using the following code. Feel free to skip this step."
      ],
      "metadata": {
        "id": "uP33cCTx2hSR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# just a single test loop\n",
        "screen = env.render(mode='rgb_array')\n",
        "plt.imshow(screen)\n",
        "\n",
        "for e in range(1):\n",
        "  last_obs = env.reset()\n",
        "\n",
        "  for i in range(500):\n",
        "    last_action = model.greedy_policy(last_obs)\n",
        "    obs, reward, done, _ = env.step(last_action)\n",
        "\n",
        "    plt.imshow(screen)\n",
        "    ipythondisplay.clear_output(wait=True)\n",
        "    ipythondisplay.display(plt.gcf())\n",
        "\n",
        "    last_obs = obs\n",
        "\n",
        "    if done:\n",
        "      print(i)\n",
        "      break"
      ],
      "metadata": {
        "id": "v6MlZCXb2o7p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## A second neural network?\n",
        "\n",
        "Maybe you already found out that learning the neural network this way might not be very stable. We are trying to match the outputs to $Q$-values that are not static. This causes a drift in the expected outputs that might cause the neural network to encounter large errors between the sampled $Q$-value $r + \\gamma \\max_{a'} Q(s', a')$ and the predicted $Q(s,a)$ one. To stabilize the learning, a common approach is to include a second network with fixed $Q$-values to use as a target reference.\n",
        "\n",
        "A very good explanation of why we need a second network is in the answer to this query:\n",
        "https://stackoverflow.com/questions/54237327/why-is-a-target-network-required.\n",
        "Also, an explanation of how the second network is used is here:\n",
        "https://www.analyticsvidhya.com/blog/2019/04/introduction-deep-q-learning-python/, or in this example: https://pytorch.org/tutorials/intermediate/reinforcement_q_learning.html\n",
        "\n",
        "Therefore, your next task is to modify your code to include a second network. Is learning faster? More stable?"
      ],
      "metadata": {
        "id": "-S3iOVo9CcT3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "0x5l4PfqI3_6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Challenge 1 - learn from images\n",
        "\n",
        "Deep reinforcement learning is often connected with learning from images directly. Our state space might be formed by the difference of images of the cart pole between the two episodes. Basically, we might capture the image of the screen in two consecutive steps and pass their difference to a **convolutional neural network** to predict the next movement. The difference captures information about the velocity of the cart, the angular velocity of the pendulum, and positional information. Your task is to take your code and modify it to work this way. Of course, you do not need to implement everything yourself; for example, feel free to copy method <code>get_screen</code> (and take other inspiration) from https://pytorch.org/tutorials/intermediate/reinforcement_q_learning.html."
      ],
      "metadata": {
        "id": "-9-eOaEBNGzP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "8Tqa3cV4NGUy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Challenge 2 - double inverted pendulum\n",
        "\n",
        "If you do not like the previous challenge, you might try a different one - a double inverted pendulum. The problem is very similar to the inverted pendulum; however, it is much harder to balance the double pendulum. Also, the state space is much larger. Therefore, you will need a larger network and more computational time.\n",
        "\n",
        "See https://github.com/openai/gym/blob/master/gym/envs/mujoco/inverted_double_pendulum.py for documentation. Note that the action is continuous; you will need to modify your network to account for infinite action space."
      ],
      "metadata": {
        "id": "ojBP-lEqOWvr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!apt-get install -y \\\n",
        "    libgl1-mesa-dev \\\n",
        "    libgl1-mesa-glx \\\n",
        "    libglew-dev \\\n",
        "    libosmesa6-dev \\\n",
        "    software-properties-common\n",
        "\n",
        "!apt-get install -y patchelf\n",
        "\n",
        "!pip install free-mujoco-py\n",
        "import mujoco_py\n",
        "\n",
        "env = gym.make('InvertedDoublePendulum-v2')"
      ],
      "metadata": {
        "id": "NOzovX6VO6dD"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}